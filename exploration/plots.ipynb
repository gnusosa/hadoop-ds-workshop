{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'1.3.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pylab inline\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xmltodict import parse as parse_xml\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from dateutil.parser import parse as parse_date\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xml_to_rdd(file_name):\n",
    "    def infer_type(input):\n",
    "        int_re = re.compile('^[\\-]?[0-9]+$')\n",
    "        date_re = re.compile('^[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}.[0-9]{3}$')\n",
    "        int_type = (int_re.match, int)\n",
    "        date_type = (date_re.match, parse_date)\n",
    "        default_type = (lambda x: True, lambda x: x)\n",
    "\n",
    "        for f, c in [int_type, date_type, default_type]:\n",
    "            if f(input):\n",
    "                return c(input)\n",
    "\n",
    "    def clean_xml_dict(xd):\n",
    "        return {\n",
    "                key[1:]: infer_type(value.strip())\n",
    "                for key, value in xd.items() if key.startswith('@')\n",
    "            }\n",
    "\n",
    "    return sc.textFile(file_name)\\\n",
    "    .filter(lambda line: line.strip().startswith('<row'))\\\n",
    "    .map(lambda line: parse_xml(line))\\\n",
    "    .map(lambda xml_dict: xml_dict['row'])\\\n",
    "    .map(clean_xml_dict)\n",
    "\n",
    "def extract_fields(row, wanted):\n",
    "    return {\n",
    "        k: row.get(k, None)\n",
    "        for k in wanted\n",
    "    }\n",
    "\n",
    "def rdd_to_df(rdd, columns, sql_context, schema=None):\n",
    "    rows = rdd.map(lambda r: extract_fields(r, columns))\\\n",
    "    .map(lambda r: Row(**r))\n",
    "    \n",
    "    return sql_context.createDataFrame(rows, schema, 0.1)\n",
    "\n",
    "def add_to_dict(input, key, value):\n",
    "    input[key] = value\n",
    "    return input\n",
    "\n",
    "data_folder = '/Users/ivoeverts/data/goto2015/se/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql_context = SQLContext(sc)\n",
    "post_age = udf(lambda dt: (datetime(2014,9,14) - dt).days, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'AcceptedAnswerId': 393,\n",
       "  u'AnswerCount': 4,\n",
       "  u'Body': u\"<p>My fianc\\xe9e and I are looking for a good Caribbean cruise in October and were wondering which islands are best to see and which Cruise line to take?</p>\\n\\n<p>It seems like a lot of the cruises don't run in this month due to Hurricane season so I'm looking for other good options.</p>\\n\\n<p><strong>EDIT</strong> We'll be travelling in 2012.</p>\",\n",
       "  u'ClosedDate': datetime.datetime(2013, 2, 25, 23, 52, 47, 953000),\n",
       "  u'CommentCount': 4,\n",
       "  u'CreationDate': datetime.datetime(2011, 6, 21, 20, 19, 34, 730000),\n",
       "  u'FavoriteCount': 1,\n",
       "  u'Id': 1,\n",
       "  u'LastActivityDate': datetime.datetime(2012, 5, 24, 14, 52, 14, 760000),\n",
       "  u'LastEditDate': datetime.datetime(2011, 12, 28, 21, 36, 43, 910000),\n",
       "  u'LastEditorUserId': 101,\n",
       "  u'OwnerUserId': 9,\n",
       "  u'PostTypeId': 1,\n",
       "  u'Score': 8,\n",
       "  u'Tags': u'<caribbean><cruising><vacation>',\n",
       "  u'Title': u'What are some Caribbean cruises for October?',\n",
       "  u'ViewCount': 309}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = xml_to_rdd(data_folder + 'Posts.xml')\n",
    "posts.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts_df = rdd_to_df(posts,\n",
    "                     ['Id', 'Score', 'CreationDate'],\n",
    "                     sql_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'CreationDate': datetime.datetime(2011, 6, 21, 0, 0),\n",
       "  u'Id': 1,\n",
       "  u'PostId': 1,\n",
       "  u'VoteTypeId': 2}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "votes = xml_to_rdd(data_folder + 'Votes.xml')\n",
    "votes.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "votes_df = rdd_to_df(votes,\n",
    "                     ['VoteTypeId', 'CreationDate', 'PostId'],\n",
    "                     sql_context).withColumnRenamed('CreationDate', 'VoteDate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts_votes_df = posts_df.join(votes_df, posts_df.Id == votes_df.PostId, 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CreationDate         Id  Score VoteDate             PostId VoteTypeId\n",
      "2011-06-21 20:48:... 31  8     2011-06-21 00:00:... 31     2         \n",
      "2011-06-21 20:48:... 31  8     2011-06-21 00:00:... 31     2         \n",
      "2011-06-21 20:48:... 31  8     2011-06-21 00:00:... 31     16        \n",
      "2011-06-21 20:48:... 31  8     2011-06-22 00:00:... 31     2         \n",
      "2011-06-21 20:48:... 31  8     2011-06-27 00:00:... 31     2         \n",
      "2011-06-21 20:48:... 31  8     2011-10-31 00:00:... 31     2         \n",
      "2011-06-21 20:48:... 31  8     2011-11-25 00:00:... 31     2         \n",
      "2011-06-21 20:48:... 31  8     2011-12-01 00:00:... 31     2         \n",
      "2011-06-21 20:48:... 31  8     2013-03-16 00:00:... 31     2         \n",
      "2011-06-21 20:48:... 31  8     2013-12-06 00:00:... 31     2         \n",
      "2011-06-21 20:48:... 31  8     2013-12-06 00:00:... 31     3         \n",
      "2011-06-22 17:43:... 231 4     2011-06-22 00:00:... 231    2         \n",
      "2011-06-22 17:43:... 231 4     2011-06-23 00:00:... 231    2         \n",
      "2011-06-22 17:43:... 231 4     2011-08-15 00:00:... 231    2         \n",
      "2011-06-22 17:43:... 231 4     2013-01-18 00:00:... 231    2         \n",
      "2011-06-24 20:10:... 431 4     2011-06-24 00:00:... 431    2         \n",
      "2011-06-24 20:10:... 431 4     2011-06-25 00:00:... 431    2         \n",
      "2011-06-24 20:10:... 431 4     2011-06-26 00:00:... 431    3         \n",
      "2011-06-24 20:10:... 431 4     2011-06-26 00:00:... 431    2         \n",
      "2011-06-24 20:10:... 431 4     2011-08-15 00:00:... 431    2         \n"
     ]
    }
   ],
   "source": [
    "posts_votes_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VoteTypeId count \n",
      "1          4661  \n",
      "2          143734\n",
      "3          3033  \n",
      "5          4207  \n",
      "6          407   \n",
      "7          35    \n",
      "8          409   \n",
      "9          404   \n",
      "10         29    \n",
      "11         58    \n",
      "12         1     \n",
      "15         952   \n",
      "16         2800  \n"
     ]
    }
   ],
   "source": [
    "posts_votes_df.groupBy('VoteTypeId').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CreationDate=datetime.datetime(2011, 6, 21, 20, 48, 29, 23000), Id=31, Score=8, VoteDate=datetime.datetime(2011, 6, 21, 0, 0), PostId=31, VoteTypeId=2),\n",
       " Row(CreationDate=datetime.datetime(2011, 6, 21, 20, 48, 29, 23000), Id=31, Score=8, VoteDate=datetime.datetime(2011, 6, 21, 0, 0), PostId=31, VoteTypeId=2),\n",
       " Row(CreationDate=datetime.datetime(2011, 6, 21, 20, 48, 29, 23000), Id=31, Score=8, VoteDate=datetime.datetime(2011, 6, 21, 0, 0), PostId=31, VoteTypeId=16),\n",
       " Row(CreationDate=datetime.datetime(2011, 6, 21, 20, 48, 29, 23000), Id=31, Score=8, VoteDate=datetime.datetime(2011, 6, 22, 0, 0), PostId=31, VoteTypeId=2),\n",
       " Row(CreationDate=datetime.datetime(2011, 6, 21, 20, 48, 29, 23000), Id=31, Score=8, VoteDate=datetime.datetime(2011, 6, 27, 0, 0), PostId=31, VoteTypeId=2)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_votes_df.rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_post_ft( (post_id, rows) ):\n",
    "    cnt = Counter()\n",
    "    cnt.update([\n",
    "            row.VoteTypeId\n",
    "            for row in rows if (row.VoteDate - row.CreationDate).days <= 7\n",
    "        ])\n",
    "    \n",
    "    return add_to_dict(dict(cnt), 'PostId', post_id)\n",
    "    \n",
    "\n",
    "ft = posts_votes_df.rdd.groupBy(lambda r: r.PostId).map(prepare_post_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{2: 5, 'PostId': 12800},\n",
       " {2: 4, 'PostId': 25600},\n",
       " {2: 3, 'PostId': 21000},\n",
       " {2: 1, 'PostId': 16400},\n",
       " {2: 1, 'PostId': 600},\n",
       " {1: 1, 2: 1, 'PostId': 24600},\n",
       " {2: 10, 'PostId': 7200},\n",
       " {2: 4, 'PostId': 7600},\n",
       " {2: 4, 'PostId': 2600},\n",
       " {2: 9, 'PostId': 5200}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 69.0 failed 1 times, most recent failure: Lost task 0.0 in stage 69.0 (TID 1033, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/ivoeverts/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py\", line 101, in main\n    process()\n  File \"/Users/ivoeverts/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py\", line 96, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/ivoeverts/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py\", line 236, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/ivoeverts/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 1220, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-8-d017e36e3f87>\", line 28, in <lambda>\nTypeError: __new__() keywords must be strings\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:744)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-365ea0a6d34a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                  \u001b[0mft_fields\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  \u001b[0msql_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                  [StructField(f, LongType(), True) for f in ft_fields]).na.fill(0)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mft_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-d017e36e3f87>\u001b[0m in \u001b[0;36mrdd_to_df\u001b[0;34m(rdd, columns, sql_context, schema)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mextract_fields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msql_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivoeverts/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 raise ValueError(\"each row in `rdd` should be list or tuple, \"\n",
      "\u001b[0;32m/Users/ivoeverts/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \"\"\"\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivoeverts/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivoeverts/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,\n\u001b[0;32m--> 842\u001b[0;31m                                           allowLocal)\n\u001b[0m\u001b[1;32m    843\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivoeverts/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivoeverts/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 69.0 failed 1 times, most recent failure: Lost task 0.0 in stage 69.0 (TID 1033, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/ivoeverts/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py\", line 101, in main\n    process()\n  File \"/Users/ivoeverts/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/worker.py\", line 96, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/ivoeverts/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/serializers.py\", line 236, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/ivoeverts/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 1220, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-8-d017e36e3f87>\", line 28, in <lambda>\nTypeError: __new__() keywords must be strings\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:744)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
     ]
    }
   ],
   "source": [
    "ft_fields = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13','15','16', 'PostId']\n",
    "ft_df = rdd_to_df(ft,\n",
    "                 ft_fields,\n",
    "                 sql_context,\n",
    "                 [StructField(f, LongType(), True) for f in ft_fields]).na.fill(0)\n",
    "ft_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?sql_context.createDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "votes_ft = sql_context.sql(\"\"\"\n",
    "select PostId, VoteTypeId, count(*) as Cnt, max(CreationDate) as MaxVoteDate\n",
    "from votes group by PostId, VoteTypeId\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts_votes_ft = posts_df.join(votes_ft, posts_df.Id == votes_ft.PostId, 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CreationDate         Id   Score PostId VoteTypeId Cnt MaxDate             \n",
      "2011-06-21 20:48:... 31   8     31     2          9   2013-12-06 00:00:...\n",
      "2011-06-21 20:48:... 31   8     31     3          1   2013-12-06 00:00:...\n",
      "2011-06-21 20:48:... 31   8     31     16         1   2011-06-21 00:00:...\n",
      "2011-06-22 17:43:... 231  4     231    2          4   2013-01-18 00:00:...\n",
      "2011-06-24 20:10:... 431  4     431    2          5   2011-10-30 00:00:...\n",
      "2011-06-24 20:10:... 431  4     431    3          1   2011-06-26 00:00:...\n",
      "2011-06-29 13:35:... 631  3     631    1          1   2011-06-29 00:00:...\n",
      "2011-06-29 13:35:... 631  3     631    2          3   2011-07-15 00:00:...\n",
      "2011-07-06 10:45:... 831  7     831    2          7   2014-08-11 00:00:...\n",
      "2011-07-16 11:49:... 1031 11    1031   2          11  2014-05-20 00:00:...\n",
      "2011-07-16 11:49:... 1031 11    1031   5          2   2012-01-19 00:00:...\n",
      "2011-07-24 16:24:... 1231 5     1231   1          1   2011-08-15 00:00:...\n",
      "2011-07-24 16:24:... 1231 5     1231   2          5   2012-04-24 00:00:...\n",
      "2011-08-05 13:40:... 1431 4     1431   1          1   2012-12-20 00:00:...\n",
      "2011-08-05 13:40:... 1431 4     1431   2          4   2012-12-20 00:00:...\n",
      "2011-08-21 18:40:... 1631 6     1631   1          1   2011-09-05 00:00:...\n",
      "2011-08-21 18:40:... 1631 6     1631   2          6   2011-09-26 00:00:...\n",
      "2011-08-29 18:47:... 1831 8     1831   2          8   2012-09-13 00:00:...\n",
      "2011-08-29 18:47:... 1831 8     1831   9          1   2011-09-04 00:00:...\n",
      "2011-09-12 14:44:... 2031 2     2031   2          2   2011-09-12 00:00:...\n"
     ]
    }
   ],
   "source": [
    "posts_votes_ft.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
